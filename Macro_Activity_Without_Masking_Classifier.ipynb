{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jPihmLbUvoa"
   },
   "source": [
    "# Train macro Classifiers without masking for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window jitter and time shift data augmentation is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWzF5-ugUvoc"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "#import cv2\n",
    "import io\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shutil\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options to set if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "# config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "# sess = tf.Session(config=config)\n",
    "# set_session(sess)  # set this TensorFlow session as the default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SDej6DfZUvol"
   },
   "outputs": [],
   "source": [
    "data_dir_train='data_2/train'\n",
    "data_dir_val= 'data_2/val'\n",
    "data_dir_test= 'data_2/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q87cb-fJUvoo"
   },
   "outputs": [],
   "source": [
    "sub_dirs = ['left_hip','right_arm','right_wrist' ] #three sensors, 9 channels total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ho3e4qQQUvor"
   },
   "outputs": [],
   "source": [
    "def parse_IMU_files(parent_dir, sub_dirs, startTime, endTime, file_name, window_length):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    data_count = 0\n",
    "    \n",
    "    for sub_dir in sub_dirs:\n",
    "        channel=[]\n",
    "        \n",
    "        for fn in glob.glob(os.path.join(parent_dir,sub_dir, file_name)):\n",
    "            file = open(fn, newline='')\n",
    "            reader = csv.reader(file)\n",
    "            first = True\n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                \n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                \n",
    "                timestamp=float(row[3]) #4th column is timestamp\n",
    "                if timestamp >=startTime and timestamp <=endTime and count<window_length:\n",
    "                    \n",
    "                    channel.append([float(row[0]),float(row[1]),float(row[2])])\n",
    "                    count = count + 1 \n",
    "                    data_count = data_count+1\n",
    "                    \n",
    "        data.append(channel)         \n",
    "    return data, data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to play with some training data extraction\n",
    "import random\n",
    "\n",
    "def parse_IMU_files_2(parent_dir, sub_dirs, startTime, endTime, file_name, window_length):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    data_count = 0\n",
    "    for sub_dir in sub_dirs:\n",
    "        channel=[]\n",
    "        \n",
    "        \n",
    "        for fn in glob.glob(os.path.join(parent_dir,sub_dir, file_name)):\n",
    "            file = open(fn, newline='')\n",
    "            reader = csv.reader(file)\n",
    "            first = True\n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                \n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                \n",
    "                timestamp=float(row[3]) #4th column is timestamp\n",
    "                \n",
    "                window_jitter1 = random.randint(-150,150)\n",
    "                \n",
    "                window_jitter2 = random.randint(-150,150)\n",
    "        \n",
    "                if timestamp >=(startTime+window_jitter1) and timestamp <=(endTime+window_jitter2) and count<window_length:\n",
    "                    \n",
    "                    channel.append([float(row[0]),float(row[1]),float(row[2])])\n",
    "                    count = count + 1  \n",
    "                    \n",
    "                    data_count = data_count+1\n",
    "                    \n",
    "        data.append(channel)         \n",
    "    return data, data_count\n",
    "\n",
    "\n",
    "def parse_IMU_files_3(parent_dir, sub_dirs, startTime, endTime, file_name, window_length):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    data_count = 0\n",
    "    for sub_dir in sub_dirs:\n",
    "        channel=[]\n",
    "                \n",
    "        window_jitter1 = random.randint(-1500,1500)\n",
    "        window_jitter2 = random.randint(-1500,1500)\n",
    "            \n",
    "        for fn in glob.glob(os.path.join(parent_dir,sub_dir, file_name)):\n",
    "            file = open(fn, newline='')\n",
    "            reader = csv.reader(file)\n",
    "            first = True\n",
    "            count = 0\n",
    "            \n",
    "            for row in reader:\n",
    "                \n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                \n",
    "                timestamp=float(row[3]) #4th column is timestamp\n",
    "    \n",
    "                if timestamp >=(startTime+window_jitter1) and timestamp <=(endTime+window_jitter2) and count<window_length:\n",
    "                    \n",
    "                    channel.append([float(row[0]),float(row[1]),float(row[2])])\n",
    "                    count = count + 1  \n",
    "                    \n",
    "                    data_count = data_count+1\n",
    "                    \n",
    "        data.append(channel)         \n",
    "    return data, data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_min_count = 100 # per sample, valid points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLINn0siUvou"
   },
   "outputs": [],
   "source": [
    "def get_train_data(data_dir, sub_dirs):\n",
    "    files = os.listdir(data_dir+'/left_hip')\n",
    "    number_of_samples = 500\n",
    "    \n",
    "    labels_macro =dict()\n",
    "    \n",
    "    labels_macro['sandwich'] = 0\n",
    "    labels_macro['fruitsalad'] = 1\n",
    "    labels_macro['cereal'] = 2\n",
    "    \n",
    "    \n",
    "    #read the labels\n",
    "    labels_loc = 'data/LabelTable.csv'\n",
    "    file_label = open(labels_loc, newline='')\n",
    "    label_reader = csv.reader(file_label)\n",
    "    file_label_mapping = dict()\n",
    "    \n",
    "    first = True\n",
    "    for row in label_reader:\n",
    "        \n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        \n",
    "        file_label_mapping[row[0]+'.csv'] = labels_macro[row[1]]\n",
    "        \n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    for f in files:\n",
    "        \n",
    "        st_index = 0\n",
    "        end_index = 30000\n",
    "        step = 1000 #overlapping window, step: 1000. \n",
    "        window_index = 10000 #6 second window\n",
    "        \n",
    "        print('reading file:',f)\n",
    "        f_name = f\n",
    "        \n",
    "        if f_name == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        curr_label_file = file_label_mapping[f_name]\n",
    "        \n",
    "        while st_index+step < end_index:\n",
    "        \n",
    "            data, data_count = parse_IMU_files(data_dir, sub_dirs, st_index, st_index+window_index,  f, number_of_samples)\n",
    "            st_index = st_index+step\n",
    "            \n",
    "            if data_count<data_min_count:\n",
    "                continue\n",
    "            \n",
    "            train_data_sample  = np.zeros((9, number_of_samples))\n",
    "            train_data_label   = curr_label_file\n",
    "            for i in range(len(data)):\n",
    "                for j in range(len(data[i])):\n",
    "                    train_data_sample[i*3,j]=data[i][j][0]\n",
    "                    train_data_sample[i*3+1,j]=data[i][j][1]\n",
    "                    train_data_sample[i*3+2,j]=data[i][j][2]\n",
    "            \n",
    "            all_data.append(train_data_sample)\n",
    "            all_labels.append(train_data_label)\n",
    "            \n",
    "    return all_data, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_2(data_dir, sub_dirs):\n",
    "    files = os.listdir(data_dir+'/left_hip')\n",
    "    number_of_samples = 500\n",
    "    \n",
    "    labels_macro =dict()\n",
    "    \n",
    "    labels_macro['sandwich'] = 0\n",
    "    labels_macro['fruitsalad'] = 1\n",
    "    labels_macro['cereal'] = 2\n",
    "    \n",
    "    \n",
    "    #read the labels\n",
    "    labels_loc = 'data/LabelTable.csv'\n",
    "    file_label = open(labels_loc, newline='')\n",
    "    label_reader = csv.reader(file_label)\n",
    "    file_label_mapping = dict()\n",
    "    \n",
    "    first = True\n",
    "    for row in label_reader:\n",
    "        \n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        \n",
    "        file_label_mapping[row[0]+'.csv'] = labels_macro[row[1]]\n",
    "        \n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    all_counts = []\n",
    "    \n",
    "    for f in files:\n",
    "        \n",
    "        st_index = 0\n",
    "        end_index = 30000\n",
    "        step = 1000 #overlapping window, step \n",
    "        window_index = 10000 #6 second window\n",
    "        \n",
    "        print('reading file:',f)\n",
    "        f_name = f\n",
    "        \n",
    "        if f_name == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        curr_label_file = file_label_mapping[f_name]\n",
    "        \n",
    "        while st_index+step < end_index:\n",
    "        \n",
    "            data, data_count = parse_IMU_files_2(data_dir, sub_dirs, st_index, st_index+window_index,  f, number_of_samples)\n",
    "            \n",
    "            st_index = st_index+step\n",
    "            \n",
    "            if data_count<data_min_count:\n",
    "                continue\n",
    "            \n",
    "            train_data_sample  = np.zeros((9, number_of_samples))\n",
    "            train_data_label   = curr_label_file\n",
    "            for i in range(len(data)):\n",
    "                for j in range(len(data[i])):\n",
    "                    train_data_sample[i*3,j]=data[i][j][0]\n",
    "                    train_data_sample[i*3+1,j]=data[i][j][1]\n",
    "                    train_data_sample[i*3+2,j]=data[i][j][2]\n",
    "            \n",
    "            all_data.append(train_data_sample)\n",
    "            all_labels.append(train_data_label)\n",
    "            \n",
    "            all_counts.append(data_count)\n",
    "            \n",
    "    return all_data, all_labels, all_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_3(data_dir, sub_dirs):\n",
    "    files = os.listdir(data_dir+'/left_hip')\n",
    "    number_of_samples = 500\n",
    "    \n",
    "    labels_macro =dict()\n",
    "    \n",
    "    labels_macro['sandwich'] = 0\n",
    "    labels_macro['fruitsalad'] = 1\n",
    "    labels_macro['cereal'] = 2\n",
    "    \n",
    "    \n",
    "    #read the labels\n",
    "    labels_loc = 'data/LabelTable.csv'\n",
    "    file_label = open(labels_loc, newline='')\n",
    "    label_reader = csv.reader(file_label)\n",
    "    file_label_mapping = dict()\n",
    "    \n",
    "    first = True\n",
    "    for row in label_reader:\n",
    "        \n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        \n",
    "        file_label_mapping[row[0]+'.csv'] = labels_macro[row[1]]\n",
    "        \n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    all_counts = []\n",
    "    \n",
    "    for f in files:\n",
    "        \n",
    "        st_index = 0\n",
    "        end_index = 30000\n",
    "        step = 1000 #overlapping window, step: 1000. \n",
    "        window_index = 10000 #6 second window\n",
    "        \n",
    "        print('reading file:',f)\n",
    "        f_name = f\n",
    "        \n",
    "        if f_name == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        curr_label_file = file_label_mapping[f_name]\n",
    "        \n",
    "        while st_index+step < end_index:\n",
    "                \n",
    "            \n",
    "            data, data_count = parse_IMU_files_3(data_dir, sub_dirs, st_index, st_index+window_index,  f, number_of_samples)\n",
    "            \n",
    "            st_index = st_index+step\n",
    "            \n",
    "            if data_count<data_min_count:\n",
    "                continue\n",
    "            \n",
    "            train_data_sample  = np.zeros((9, number_of_samples))\n",
    "            train_data_label   = curr_label_file\n",
    "            for i in range(len(data)):\n",
    "                for j in range(len(data[i])):\n",
    "                    train_data_sample[i*3,j]=data[i][j][0]\n",
    "                    train_data_sample[i*3+1,j]=data[i][j][1]\n",
    "                    train_data_sample[i*3+2,j]=data[i][j][2]\n",
    "            \n",
    "            all_data.append(train_data_sample)\n",
    "            all_labels.append(train_data_label)\n",
    "            \n",
    "            all_counts.append(data_count)\n",
    "            \n",
    "            \n",
    "            \n",
    "    return all_data, all_labels, all_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEYY86nqUvoy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#receive windowed training and validation data\n",
    "train_x, train_y = get_train_data(data_dir_train,sub_dirs)\n",
    "val_x, val_y = get_train_data(data_dir_val,sub_dirs)\n",
    "test_x, test_y = get_train_data(data_dir_test,sub_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x2, train_y2, all_counts = get_train_data_2(data_dir_train,sub_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x3, train_y3, all_counts = get_train_data_3(data_dir_train,sub_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4862, 9, 500)\n",
      "(4862,)\n"
     ]
    }
   ],
   "source": [
    "train_samples_2 = np.array(train_x2) \n",
    "train_labels2_2 = np.array(train_y2)\n",
    "print(train_samples_2.shape)\n",
    "print(train_labels2_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4856, 9, 500)\n",
      "(4856,)\n"
     ]
    }
   ],
   "source": [
    "train_samples_3 = np.array(train_x3) \n",
    "train_labels2_3 = np.array(train_y3)\n",
    "print(train_samples_3.shape)\n",
    "print(train_labels2_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u2w_y9dmUvo1"
   },
   "outputs": [],
   "source": [
    "train_samples = np.array(train_x) \n",
    "train_labels2 = np.array(train_y)\n",
    "val_samples = np.array(val_x) \n",
    "val_labels2 = np.array(val_y)\n",
    "test_samples = np.array(test_x) \n",
    "test_labels2 = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4864, 9, 500)\n",
      "(4864,)\n",
      "(1348, 9, 500)\n",
      "(1348,)\n",
      "(1356, 9, 500)\n",
      "(1356,)\n"
     ]
    }
   ],
   "source": [
    "print(train_samples.shape)\n",
    "print(train_labels2.shape)\n",
    "print(val_samples.shape)\n",
    "print(val_labels2.shape)\n",
    "print(test_samples.shape)\n",
    "print(test_labels2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = np.vstack((train_samples,train_samples_2, train_samples_3))\n",
    "train_labels2 = np.hstack((train_labels2, train_labels2_2, train_labels2_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNUKXbk9Uvo3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4864, 9, 500)\n",
      "(4864,)\n",
      "(1348, 9, 500)\n",
      "(1348,)\n",
      "(1356, 9, 500)\n",
      "(1356,)\n"
     ]
    }
   ],
   "source": [
    "print(train_samples.shape)\n",
    "print(train_labels2.shape)\n",
    "print(val_samples.shape)\n",
    "print(val_labels2.shape)\n",
    "print(test_samples.shape)\n",
    "print(test_labels2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1OiMsBOUvo6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1356, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert to one hot encoding\n",
    "from keras.utils  import to_categorical\n",
    "train_labels = to_categorical(train_labels2)\n",
    "train_labels.shape\n",
    "val_labels = to_categorical(val_labels2)\n",
    "val_labels.shape\n",
    "test_labels = to_categorical(test_labels2)\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_d-H-X42Uvo8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4864, 9, 500)\n",
      "(4864, 3)\n",
      "(1348, 9, 500)\n",
      "(1348, 3)\n",
      "(1356, 9, 500)\n",
      "(1356,)\n"
     ]
    }
   ],
   "source": [
    "print(train_samples.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_samples.shape)\n",
    "print(val_labels.shape)\n",
    "print(test_samples.shape)\n",
    "print(test_labels2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wqxij4vFUvo_"
   },
   "outputs": [],
   "source": [
    "#For CNN and BidirLSTM:\n",
    "number_of_samples = 500\n",
    "train_samples = train_samples.reshape((-1, 9,number_of_samples, 1))\n",
    "val_samples = val_samples.reshape((-1, 9,number_of_samples, 1))\n",
    "test_samples = test_samples.reshape((-1, 9,number_of_samples, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rf0eewtJUvpB"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyNXsGAAUvpC"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, Flatten, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Permute, Reshape\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKNc7WZQUvpE"
   },
   "outputs": [],
   "source": [
    "def _data_reshaping(X_tr, X_va, X_ts, network_type):\n",
    "    _, win_len, dim = X_tr.shape\n",
    "    print(network_type)\n",
    "    if network_type=='CNN' or network_type=='ConvLSTM':\n",
    "\n",
    "        X_tr = np.swapaxes(X_tr,1,2)\n",
    "        X_va = np.swapaxes(X_va,1,2)\n",
    "        X_ts = np.swapaxes(X_ts,1,2)\n",
    "\n",
    "        X_tr = np.reshape(X_tr, (-1, dim, win_len, 1))\n",
    "        X_va = np.reshape(X_va, (-1, dim, win_len, 1))\n",
    "        X_ts = np.reshape(X_ts, (-1, dim, win_len, 1))\n",
    "    \n",
    "    return X_tr, X_va, X_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HcMCdtFUUvpF"
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "def model_variant(model, num_feat_map, dim, network_type,p):\n",
    "    print(network_type)\n",
    "    if network_type == 'ConvLSTM':\n",
    "        model.add(Permute((2, 1, 3))) \n",
    "        model.add(Reshape((-1,num_feat_map*dim)))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=False, stateful=False)))\n",
    "    if network_type == 'CNN':\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(BatchNormalization()) \n",
    "        model.add(Dropout(p))\n",
    "\n",
    "        \n",
    "def model_conv(model, num_feat_map,p,b):\n",
    "    model.add(Conv2D(num_feat_map, kernel_size=(1, 10),    \n",
    "                 activation='relu',\n",
    "                 input_shape=(dim, win_len, 1),\n",
    "                 padding='same'))\n",
    "    \n",
    "    model.add(Conv2D(num_feat_map, kernel_size=(1, 10), activation='relu',padding='same'))\n",
    "    \n",
    "    if (b==1):\n",
    "        model.add(BatchNormalization()) \n",
    "    model.add(Conv2D(num_feat_map, kernel_size=(1, 10), activation='relu',padding='same'))\n",
    "    \n",
    "    if (b==1):\n",
    "        model.add(BatchNormalization()) \n",
    "    model.add(MaxPooling2D(pool_size=(1, 3)))\n",
    "    \n",
    "    model.add(Conv2D(num_feat_map, kernel_size=(1, 10), activation='relu',padding='same')) \n",
    "    model.add(Conv2D(num_feat_map, kernel_size=(1, 10), activation='relu',padding='same'))\n",
    "    if (b==1):\n",
    "        model.add(BatchNormalization()) \n",
    "    model.add(MaxPooling2D(pool_size=(1, 2)))\n",
    "    model.add(Dropout(p))\n",
    "    \n",
    "    model.add(Conv2D(num_feat_map, kernel_size=(1, 10), activation='relu',padding='same'))  \n",
    "    if (b==1):\n",
    "        model.add(BatchNormalization()) \n",
    "    model.add(MaxPooling2D(pool_size=(1, 2)))\n",
    "    \n",
    "    model.add(Dropout(p))\n",
    "    \n",
    "def model_LSTM(model,p):\n",
    "    model.add(LSTM(num_hidden_lstm, \n",
    "               input_shape=(win_len,dim), \n",
    "               return_sequences=True))\n",
    "    model.add(Dropout(p))\n",
    "    model.add(LSTM(num_hidden_lstm, return_sequences=False))\n",
    "    model.add(Dropout(p))\n",
    "    \n",
    "def model_output(model):\n",
    "    model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wf8LuDxlUvpH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_feat_map = 128\n",
    "num_hidden_lstm = 128\n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "network_type = 'ConvLSTM'\n",
    "_, dim, win_len,_ = train_samples.shape\n",
    "\n",
    "print(win_len)\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fAQRf1DNUvpK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building the model ... \n",
      "WARNING:tensorflow:From /home/sandeep/anaconda3/envs/sandeep-env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/sandeep/anaconda3/envs/sandeep-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "ConvLSTM\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 9, 500, 128)       1408      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 500, 128)       163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 9, 500, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 9, 500, 128)       163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 500, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 166, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 166, 128)       163968    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 166, 128)       163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 9, 166, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 83, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 83, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 9, 83, 128)        163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 9, 83, 128)        512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 41, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9, 41, 128)        0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 41, 9, 128)        0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 41, 1152)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               1311744   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 2,135,811\n",
      "Trainable params: 2,134,787\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "p=0.5 #Dropout\n",
    "b = 1 #BatchNorm\n",
    "print('building the model ... ')\n",
    "model = Sequential()\n",
    "\n",
    "if network_type=='CNN' or network_type=='ConvLSTM':\n",
    "    model_conv(model, num_feat_map,p,b)\n",
    "    model_variant(model, num_feat_map, dim, network_type,p)\n",
    "if network_type=='LSTM':\n",
    "    model_LSTM(model,p)\n",
    "       \n",
    "model_output(model)    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wrNCEAD8UvpL"
   },
   "outputs": [],
   "source": [
    "X_train = train_samples\n",
    "y_train = train_labels\n",
    "X_val = val_samples\n",
    "y_val = val_labels\n",
    "X_test = test_samples\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNsDOBNrUvpN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4864, 9, 500, 1) (1348, 9, 500, 1) (1356, 9, 500, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "class monitor_Training(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = np.argmax(self.model.predict(X_test), axis=1)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        print('test accuracy is:', accuracy)\n",
    "        return\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint /home/sandeep/data/DCBL_macro_TTV_4.hdf5\n",
    "filepath=\"macro_without_masking.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YpsJRPEfUvpP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "\n",
    "checkpoint2 = monitor_Training()\n",
    "\n",
    "callbacks_list = [checkpoint, checkpoint2]\n",
    "\n",
    "H = model.fit(train_samples, train_labels,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            validation_data=(X_val, y_val),\n",
    "             callbacks=callbacks_list\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ASUmtFSUvpQ"
   },
   "outputs": [],
   "source": [
    "history = H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C6yi5aaJUvpS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "print(np.argmax(np.array(history.history['val_accuracy'])))\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "print('Maximum validation accuracy: ',np.max(np.array(history.history['val_accuracy'])))\n",
    "print('Training accuracy of best model: ',np.array(history.history['accuracy'])[np.argmax(np.array(history.history['val_accuracy']))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sandeep/anaconda3/envs/sandeep-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "[[570  39  23]\n",
      " [ 76 277  24]\n",
      " [ 93  62 192]]\n",
      "the mean-f1 score: 0.74\n",
      "accuracy is: 0.77\n"
     ]
    }
   ],
   "source": [
    "model = load_model(filepath)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(cf_matrix)\n",
    "class_wise_f1 = np.round(f1_score(y_true, y_pred, average=None)*100)*0.01\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "print('the mean-f1 score: {:.2f}'.format(np.mean(class_wise_f1)))\n",
    "print('accuracy is: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File by file the Macro comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data windows per file and see their predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_prediction_accuracy(data_dir, sub_dirs):\n",
    "\n",
    "    files = os.listdir(data_dir+'/left_hip')\n",
    "    number_of_samples = 500\n",
    "\n",
    "    labels_macro =dict()\n",
    "\n",
    "    labels_macro['sandwich'] = 0\n",
    "    labels_macro['fruitsalad'] = 1\n",
    "    labels_macro['cereal'] = 2\n",
    "\n",
    "\n",
    "    #read the labels\n",
    "    labels_loc = 'data/LabelTable.csv'\n",
    "    file_label = open(labels_loc, newline='')\n",
    "    label_reader = csv.reader(file_label)\n",
    "    file_label_mapping = dict()\n",
    "\n",
    "    first = True\n",
    "    for row in label_reader:\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "\n",
    "        file_label_mapping[row[0]+'.csv'] = labels_macro[row[1]]\n",
    "\n",
    "\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "\n",
    "    for f in files:\n",
    "\n",
    "        file_data = []\n",
    "        file_label = []\n",
    "\n",
    "        st_index = 0\n",
    "        end_index = 30000\n",
    "        step = 1000 #overlapping window, step: 1000. \n",
    "        window_index = 10000 #6 second window\n",
    "\n",
    "        #print('reading file:',f)\n",
    "        f_name = f\n",
    "\n",
    "        if f_name == '.DS_Store':\n",
    "            continue\n",
    "\n",
    "        total_count = total_count+1\n",
    "\n",
    "        curr_label_file = file_label_mapping[f_name]\n",
    "\n",
    "        while st_index+step < end_index:\n",
    "\n",
    "            data, data_count = parse_IMU_files(data_dir, sub_dirs, st_index, st_index+window_index,  f, number_of_samples)\n",
    "            st_index = st_index+step\n",
    "\n",
    "            if data_count<data_min_count:\n",
    "                continue\n",
    "\n",
    "            train_data_sample  = np.zeros((9, number_of_samples))\n",
    "            train_data_label   = curr_label_file\n",
    "            for i in range(len(data)):\n",
    "                for j in range(len(data[i])):\n",
    "                    train_data_sample[i*3,j]=data[i][j][0]\n",
    "                    train_data_sample[i*3+1,j]=data[i][j][1]\n",
    "                    train_data_sample[i*3+2,j]=data[i][j][2]\n",
    "\n",
    "            file_data.append(train_data_sample)\n",
    "            #file_label.append(train_data_label)\n",
    "\n",
    "        file_data = np.array(file_data)\n",
    "        file_label = curr_label_file\n",
    "\n",
    "        file_data = file_data.reshape((-1, 9,500, 1))\n",
    "\n",
    "        y_pred = np.argmax(model.predict(file_data), axis=1) \n",
    "        counts = np.bincount(y_pred)\n",
    "        prediction = np.argmax(counts) #max occuring value in windows\n",
    "        \n",
    "        #correct prediction\n",
    "        if int(prediction)==int(file_label):\n",
    "            correct_count = correct_count+1\n",
    "        \n",
    "    \n",
    "    return total_count, correct_count        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count of test files: 50\n",
      "correctly predicted test files: 42\n",
      "Percentage accuracy: 84.0\n"
     ]
    }
   ],
   "source": [
    "# Test data accuracy:\n",
    "total_count, correct_count = get_prediction_accuracy(data_dir_test, sub_dirs)\n",
    "print('total_count of test files:', total_count)\n",
    "print('correctly predicted test files:', correct_count)\n",
    "print('Percentage accuracy:', (correct_count/total_count)*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count of train files: 175\n",
      "correctly predicted train files: 175\n",
      "Percentage accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "# Train accuracy\n",
    "total_count, correct_count = get_prediction_accuracy(data_dir_train, sub_dirs)\n",
    "print('total_count of train files:', total_count)\n",
    "print('correctly predicted train files:', correct_count)\n",
    "print('Percentage accuracy:', (correct_count/total_count)*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count of val files: 48\n",
      "correctly predicted val files: 45\n",
      "Percentage accuracy: 93.75\n"
     ]
    }
   ],
   "source": [
    "#val accuracy\n",
    "total_count, correct_count = get_prediction_accuracy(data_dir_val, sub_dirs)\n",
    "print('total_count of val files:', total_count)\n",
    "print('correctly predicted val files:', correct_count)\n",
    "print('Percentage accuracy:', (correct_count/total_count)*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Macro-Activity",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
